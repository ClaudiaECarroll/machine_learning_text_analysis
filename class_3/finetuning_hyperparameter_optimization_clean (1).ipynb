{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4TDpbSuZFXYO",
      "metadata": {
        "id": "4TDpbSuZFXYO"
      },
      "source": [
        "### This tutorial walks through the basic steps of fine-tuning a pretrained BERT classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cff85e0",
      "metadata": {
        "id": "1cff85e0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167a4dc3",
      "metadata": {
        "id": "167a4dc3"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import random\n",
        "import itertools\n",
        "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cm06uOkUODlB",
      "metadata": {
        "id": "Cm06uOkUODlB"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1394429-8a70-4d94-944f-9b98bfa12937",
      "metadata": {
        "id": "c1394429-8a70-4d94-944f-9b98bfa12937"
      },
      "source": [
        "### Setting hyperparamters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "835aaf12",
      "metadata": {
        "id": "835aaf12"
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters\n",
        "\n",
        "MODEL_NAME = 'bert-base-uncased'\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "MAX_LENGTH = 300\n",
        "NUM_EPOCHS = 4\n",
        "TRAINING_DATA_SIZE = 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ook99ZKLOWAL",
      "metadata": {
        "id": "ook99ZKLOWAL"
      },
      "source": [
        "### Loading and preparing the training data we will be using to fine-tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14f2dfe",
      "metadata": {
        "id": "c14f2dfe"
      },
      "outputs": [],
      "source": [
        "# Load a smaller subset of the IMDB dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ClaudiaECarroll/Teaching_Materials/main/IMDB_Dataset.csv\")\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jpxX4Yr2F6j5",
      "metadata": {
        "id": "jpxX4Yr2F6j5"
      },
      "outputs": [],
      "source": [
        "#Take a look at the data to see how it is structured.\n",
        "df = df.sample(n=TRAINING_DATA_SIZE, random_state=42).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZffjTLvSb9rr",
      "metadata": {
        "id": "ZffjTLvSb9rr"
      },
      "outputs": [],
      "source": [
        "#Take a look at our random sample\n",
        "df[\"sentiment\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ypBCqcO19va",
      "metadata": {
        "id": "5ypBCqcO19va"
      },
      "outputs": [],
      "source": [
        "#Extract out the texts to a list for later\n",
        "\n",
        "texts = df['review'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jAqzXl8I1rkf",
      "metadata": {
        "id": "jAqzXl8I1rkf"
      },
      "outputs": [],
      "source": [
        "# Create a list that tracks the postive and negative labels in numeric form.\n",
        "\n",
        "designation_numeric = []\n",
        "\n",
        "#df['designation']\n",
        "\n",
        "for x in df['sentiment']:\n",
        "    if x == 'positive':\n",
        "        designation_numeric.append(1)\n",
        "    elif x == 'negative':\n",
        "        designation_numeric.append(0)\n",
        "    else:\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de07dfdc-7c81-4bbb-b943-2fa1aeac4be1",
      "metadata": {
        "id": "de07dfdc-7c81-4bbb-b943-2fa1aeac4be1"
      },
      "outputs": [],
      "source": [
        "#Convert the numeric labels to a TensorFlow object\n",
        "\n",
        "labels = torch.tensor(designation_numeric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b445f2",
      "metadata": {
        "id": "56b445f2"
      },
      "outputs": [],
      "source": [
        "# Convert the text labels to numeric in the dataframe to keep track.\n",
        "\n",
        "label_map = {'positive': 1, 'negative': 0}\n",
        "df['label'] = df['sentiment'].map(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x_en0dDEOb4C",
      "metadata": {
        "id": "x_en0dDEOb4C"
      },
      "source": [
        "### Preparing the training data for processing by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6CpQbBoB3OrS",
      "metadata": {
        "id": "6CpQbBoB3OrS"
      },
      "outputs": [],
      "source": [
        "#Setting up the train/text split\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_7DGDkfl3OU-",
      "metadata": {
        "id": "_7DGDkfl3OU-"
      },
      "outputs": [],
      "source": [
        "# Calling the tokenizer for Bert from Hugging Face Transformers\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afE22BPr3jiz",
      "metadata": {
        "id": "afE22BPr3jiz"
      },
      "outputs": [],
      "source": [
        "# Creating the encodings (tokens) to represent our texts numerically for processing by the model.\n",
        "\n",
        "train_encodings = tokenizer(\n",
        "    list(train_texts),\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vMvw5T0s4uRE",
      "metadata": {
        "id": "vMvw5T0s4uRE"
      },
      "outputs": [],
      "source": [
        "# Doing the same as above for the test set\n",
        "\n",
        "val_encodings = tokenizer(\n",
        "    list(val_texts),\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "\n",
        "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89195e55-4354-4811-b5ea-0369fc0f483b",
      "metadata": {
        "id": "89195e55-4354-4811-b5ea-0369fc0f483b"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at what the encodings look like\n",
        "\n",
        "train_dataset[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U2kGLOqX3jfb",
      "metadata": {
        "id": "U2kGLOqX3jfb"
      },
      "outputs": [],
      "source": [
        "# Calling the DataLoader method fro Hugging Face transformers to manage our data being passed to the model\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZVRAbK_SOq6y",
      "metadata": {
        "id": "ZVRAbK_SOq6y"
      },
      "source": [
        "## Loading the Pre-Trained model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce2db54",
      "metadata": {
        "id": "dce2db54"
      },
      "outputs": [],
      "source": [
        "# Load model from Hugging Face\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tQkbU2i96MXO",
      "metadata": {
        "id": "tQkbU2i96MXO"
      },
      "outputs": [],
      "source": [
        "# Setting the optimizer, and learning rate scheduler setting to optimize training (math stuff)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Y8fy6XuPHZR",
      "metadata": {
        "id": "8Y8fy6XuPHZR"
      },
      "outputs": [],
      "source": [
        "# Set loss function to track accuracy loss during training\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w5wTbEqVP5P7",
      "metadata": {
        "id": "w5wTbEqVP5P7"
      },
      "outputs": [],
      "source": [
        "# Before we actually start fine-lets check if the GPU is available (not, you need to select T4 GPU by going to Runtime --> Change Runtime Type)\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "161fd912-3de3-462d-ad77-648a480efc0a",
      "metadata": {
        "id": "161fd912-3de3-462d-ad77-648a480efc0a"
      },
      "source": [
        "### Actually doing the fine-tuning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mr-kEwX_0LOx",
      "metadata": {
        "id": "mr-kEwX_0LOx"
      },
      "outputs": [],
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # TRAINING\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    # EVALUATION\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actual_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            _, preds = torch.max(logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    accuracy = accuracy_score(actual_labels, predictions)\n",
        "    report = classification_report(actual_labels, predictions)\n",
        "\n",
        "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "    print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WuAWwbTYSUkb",
      "metadata": {
        "id": "WuAWwbTYSUkb"
      },
      "source": [
        "#### And now we wait...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying the Fine-Tuned Model"
      ],
      "metadata": {
        "id": "ONtJa31zJw-0"
      },
      "id": "ONtJa31zJw-0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n9Bj66PF-40W",
      "metadata": {
        "id": "n9Bj66PF-40W"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, model, tokenizer, device, max_length=300):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)  # <-- fixed here\n",
        "    return \"positive\" if preds.item() == 1 else \"negative\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44rnd48X-4xy",
      "metadata": {
        "id": "44rnd48X-4xy"
      },
      "outputs": [],
      "source": [
        "test_text = \"This was a terrible movie!.\"\n",
        "sentiment = predict_sentiment(test_text, model, tokenizer, device)\n",
        "print(test_text)\n",
        "print(f\"Predicted sentiment: {sentiment}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dbF9PAH_9xr",
      "metadata": {
        "id": "2dbF9PAH_9xr"
      },
      "source": [
        "## Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a176fecf-e522-4d2a-9a75-e8d65786f95d",
      "metadata": {
        "id": "a176fecf-e522-4d2a-9a75-e8d65786f95d"
      },
      "outputs": [],
      "source": [
        "# Define hyperparameter grid\n",
        "\n",
        "learning_rates = [2e-5, 3e-5]\n",
        "batch_sizes = [16, 32]\n",
        "epochs = [2]  # keep small for demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47d32e81-75db-4986-9c92-443625cb0f87",
      "metadata": {
        "id": "47d32e81-75db-4986-9c92-443625cb0f87"
      },
      "outputs": [],
      "source": [
        "# Cartesian product of all combinations\n",
        "param_grid = list(itertools.product(learning_rates, batch_sizes, epochs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b44cda-aff5-451c-968f-769e0ba4cb58",
      "metadata": {
        "id": "e2b44cda-aff5-451c-968f-769e0ba4cb58"
      },
      "outputs": [],
      "source": [
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for i, (lr, bs, num_epochs) in enumerate(param_grid):\n",
        "    print(f\"\\nTrial {i+1}/{len(param_grid)} — LR={lr}, Batch Size={bs}, Epochs={num_epochs}\")\n",
        "\n",
        "    # Re-initialize model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
        "\n",
        "    # Dataloaders with current batch size\n",
        "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=bs)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "    # Training loop (simplified)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            loss = nn.CrossEntropyLoss()(outputs.logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[0].to(device)\n",
        "            attention_mask = batch[1].to(device)\n",
        "            labels = batch[2].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            predictions.extend(preds.cpu().tolist())\n",
        "            actuals.extend(labels.cpu().tolist())\n",
        "\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "\n",
        "    if acc > best_accuracy:\n",
        "        best_accuracy = acc\n",
        "        best_params = (lr, bs, num_epochs)\n",
        "\n",
        "print(f\"\\n🏆 Best Validation Accuracy: {best_accuracy:.4f} with params: LR={best_params[0]}, Batch Size={best_params[1]}, Epochs={best_params[2]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76578477-976b-4769-a305-395078f0e381",
      "metadata": {
        "id": "76578477-976b-4769-a305-395078f0e381"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}